{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmnPr6xlkEZgY8s4riXsrT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rssubramaniyan1/EVA8/blob/main/EVA8_Assignment5_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Normalization function**\n",
        "\n",
        "The below code defines three types of normalization Group, Batch and Layer normalization.\n",
        "\n",
        "The below are the links to the papers.\n",
        "\n",
        "Layer Normilzation  : https://arxiv.org/pdf/1607.06450.pdf\n",
        "\n",
        "Group Normalization : https://arxiv.org/pdf/1803.08494.pdf\n",
        "\n",
        "Batch Normalization : https://arxiv.org/pdf/1502.03167.pdf\n",
        "\n",
        "This normalization function is called in the network (taken from assignment4 best model- attempt 2)\n",
        "\n",
        "# Batch Normalization:\n",
        "\n",
        "> Introduced to tackle internal covariate shift. During training as the parameters are learnt from one layer optimised for that layer, while the input in the next layer are different. It causes the slowing down of the training process of a DNN due to slower convergence. \n",
        "\n",
        "> BN allows to speed up the process by normalising the input to layers therby ensuring not too much shift in the distribution of inputs between layers leading to faster convergence \n",
        "\n",
        "> The mean and standard deviation used for normalization are computed from the data set before the taining the network\n",
        "\n",
        ">In BN the pixels sharing the same channel index are normalized together, ie for each channel, BN computes mean and standard dev.\n",
        "\n",
        "# Group Normalization:\n",
        "\n",
        ">This approach takes the channels divides them into groups and then for each group computes the mean and variance that is used for normalization. \n",
        "\n",
        ">The GN is independent of the batch size. \n",
        "\n",
        ">GN the mean and standard dev are computed across channels in a given group\n",
        "\n",
        "> The groups are obtainted by channel/G (G = 32 is the hyper parameter)\n",
        "\n",
        "> if G=1 then GN becomes layer normalization\n",
        "\n",
        "\n",
        "# Layer Normalization\n",
        "\n",
        "> Takes all channels in a current epoch\n",
        "\n",
        "> Compute the Mean and Standard Dev across the channels \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Uy-uUUoTFniH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7OPEkFWE_Cr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# define a class with an attribute and three methods for group normalization, layer normalization and L1 Norm Batch normalization\n",
        "class Normalization:\n",
        "    def __init__(self, norm_type):\n",
        "        self.norm_type = norm_type\n",
        "\n",
        "    def group_norm(self, x, num_channels, num_groups=32, eps=1e-5,gamma=1, beta=0):\n",
        "        x = x.view(num_groups, -1, num_channels // num_groups).contiguous()\n",
        "        # contigous() is used to make sure that the tensor is stored in a contiguous block of memory\n",
        "        # it is not a copy of the original tensor\n",
        "\n",
        "        # calculate the mean and standard deviation along the second dimension # what is the second dimension with respect to?\n",
        "        # the second dimension is the dimension of the input channels\n",
        "        # the second dimension is the dimension along which the mean and standard deviation are calculated\n",
        "\n",
        "        mean = x.mean(1, keepdim=True) # the 1 in mean(1, keepdim=True) indicates that the mean is calculated along the second dimension\n",
        "        std = x.std(1, keepdim=True)\n",
        "        # normalize the input tensor\n",
        "        x = (x - mean) / (std + 1e-5)\n",
        "        # reshape the input tensor to the original shape\n",
        "        x = x.view(-1, num_channels).contiguous()\n",
        "        # how to learn gamma and beta?\n",
        "        # gamma and beta are initialized to 1 and 0 respectively\n",
        "        # and are learned using backpropagation\n",
        "        # requires_grad=True is used to learn gamma and beta\n",
        "        # requires_grad = True makes the tensor a leaf node in the computation graph;\n",
        "        # it is a tensor that requires gradient computation\n",
        "        # when we call backward() on the loss function, the gradient of the loss function with respect to the leaf nodes is computed\n",
        "        #\n",
        "        gamma = torch.ones(num_channels, requires_grad=True)\n",
        "        beta = torch.zeros(num_channels, requires_grad=True) # requires_grad=True is used to learn gamma and beta\n",
        "\n",
        "        return x * gamma + beta\n",
        "\n",
        "    def layer_norm(self, x, num_channels, eps=1e-5,gamma=1, beta=0):\n",
        "        # Layer Normalization\n",
        "        # https://arxiv.org/pdf/1607.06450.pdf\n",
        "        x = x.view(-1, num_channels).contiguous() # view() is used to reshape the input tensor to the desired shape\n",
        "        # calculate the mean and standard deviation along the second dimension\n",
        "        mean = x.mean(1, keepdim=True)\n",
        "        std = x.std(1, keepdim=True)\n",
        "        # normalize the input tensor\n",
        "        x = (x - mean) / (std + eps)\n",
        "        # how to learn gamma and beta?\n",
        "        # gamma and beta are initialized to 1 and 0 respectively\n",
        "        # and are learned using backpropagation\n",
        "        gamma = torch.ones(num_channels, requires_grad=True)\n",
        "        beta = torch.zeros(num_channels, requires_grad=True)\n",
        "\n",
        "        return x * gamma + beta\n",
        "\n",
        "    def l1_norm_bn(self, x, num_channels, eps=1e-5,gamma=1, beta=0):\n",
        "        # L1 Normalization\n",
        "        # norm is calculated as the sum of absolute values of the input tensor\n",
        "        # divided by the number of channels\n",
        "        mean = x.abs().sum(1, keepdim=True) / num_channels # what is the 1 in sum(1, keepdim=True) indicating?\n",
        "        # normalize the input tensor\n",
        "        x = x / (mean + eps)\n",
        "        gamma = torch.ones(num_channels, requires_grad=True)\n",
        "        beta = torch.zeros(num_channels, requires_grad=True)\n",
        "        return x * gamma + beta\n",
        "\n",
        "    def __call__(self, x, num_channels):\n",
        "        if self.norm_type == 'GN':\n",
        "            return self.group_norm(x, num_channels)\n",
        "        elif self.norm_type == 'LN':\n",
        "            return self.layer_norm(x, num_channels)\n",
        "        elif self.norm_type == 'BN':\n",
        "            return self.l1_norm_bn(x, num_channels)\n",
        "        else:\n",
        "            return x"
      ]
    }
  ]
}