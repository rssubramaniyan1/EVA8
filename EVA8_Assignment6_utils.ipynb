{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlp/uJ5D2N9NMywdLGq43M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rssubramaniyan1/EVA8/blob/main/EVA8_Assignment6_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFIFTcHV24GU"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to calculate mean and standard deviation of the dataset\n",
        "\n",
        "def calculate_mean_std(dataset):\n",
        "    # shape (50000, 32, 32, 3)\n",
        "    imgs = np.concatenate(dataset, axis=0)/255.0\n",
        "    # the red channel across all images\n",
        "    r = imgs[:, :, 0].mean()\n",
        "    # the green channel across all images\n",
        "    g = imgs[:, :, 1].mean()\n",
        "    # the blue channel across all images\n",
        "    b = imgs[:, :, 2].mean()\n",
        "    # the red channel across all images\n",
        "    r_std = imgs[:, :, 0].std()\n",
        "    # the green channel across all images\n",
        "    g_std = imgs[:, :, 1].std()\n",
        "    # the blue channel across all images\n",
        "    b_std = imgs[:, :, 2].std()\n",
        "    return [r, g, b], [r_std, g_std, b_std]"
      ],
      "metadata": {
        "id": "iUid7_VJ3QZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply transformations on the train dataset\n",
        "\n",
        "def apply_train_transforms(train_set, mean_train, std_train, batch_size, kwargs):\n",
        "    train_transforms = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "            A.CoarseDropout(max_holes=1, max_height=16, max_width=16, min_holes=1, min_height=16, min_width=16,\n",
        "                        fill_value=(mean_train), p=0.5),\n",
        "            A.Normalize(mean_train, std_train),\n",
        "            ToTensorV2()\n",
        "            ])\n",
        "\n",
        "    # Applying the transformations on the train dataset and creating a train loader\n",
        "    # reshape to HWC format\n",
        "    train_dataset = train_set.data.transpose((0, 1, 2, 3))\n",
        "    train_labels = train_set.targets\n",
        "    # get the train_dataset in the required format for albumentations library to work cv2 format\n",
        "    #train_dataset = np.array([cv2.cvtColor(train_dataset[p], cv2.COLOR_BGR2RGB) for p in range(train_dataset.shape[0])])\n",
        "    train_dataset=[train_transforms(image=train_dataset[p]) for p in range(train_dataset.shape[0])]\n",
        "    # Convert the list of dictionaries to a torch.Tensor\n",
        "    train_dataset = torch.stack([x['image'] for x in train_dataset])\n",
        "    # Transpose the tensor to the required shape (N, C, H, W) i.e. (50000, 3, 32, 32)\n",
        "    train_dataset = train_dataset.transpose(2, 1).transpose(1,2)\n",
        "    train_dataset = torch.utils.data.TensorDataset(train_dataset, torch.tensor(train_labels))\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "    return train_loader\n"
      ],
      "metadata": {
        "id": "SPl8j3WO3T-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def apply_test_transforms(test_set, mean_test, std_test, batch_size, kwargs):\n",
        "    test_transforms = A.Compose([\n",
        "            A.Normalize(mean_test, std_test),\n",
        "            ToTensorV2()\n",
        "            ])\n",
        "    # Applying the transformations on the test dataset and creating a test loader\n",
        "    test_dataset = test_set.data.transpose((0, 1, 2, 3))\n",
        "    test_labels = test_set.targets\n",
        "    test_dataset = [test_transforms(image = test_dataset[p]) for p in range(test_dataset.shape[0])]\n",
        "    # Convert the list of dictionaries to a torch.Tensor\n",
        "    test_dataset = torch.stack([x['image'] for x in test_dataset])\n",
        "    # transpose not required as the test dataset is already in the required shape (N, C, H, W) i.e. (10000, 3, 32, 32)\n",
        "    test_dataset = torch.utils.data.TensorDataset(test_dataset, torch.tensor(test_labels))\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "    return test_loader\n"
      ],
      "metadata": {
        "id": "RKB-LrGU3XiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# write a function for the optimizer and scheduler\n",
        "\n",
        "def get_optimizer(model,learning_rate,momentum,weight_decay,step_size,gamma):# Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "    return criterion, optimizer, scheduler\n"
      ],
      "metadata": {
        "id": "I31KT7-13Zq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_losses = []\n",
        "train_acc = []\n",
        "test_losses = []\n",
        "test_acc = []\n",
        "\n"
      ],
      "metadata": {
        "id": "I7HiiJah3b43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        # get samples\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Init\n",
        "        optimizer.zero_grad()\n",
        "        # Predict\n",
        "        y_pred = model(data)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = F.nll_loss(y_pred, target)\n",
        "        train_losses.append(loss)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update pbar-tqdm\n",
        "        pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        pbar.set_description(desc= f'Loss={loss.item():0.2f} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "        train_acc.append(100*correct/processed)"
      ],
      "metadata": {
        "id": "xm66aWsL3fxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "kzS8wcRw3glH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}