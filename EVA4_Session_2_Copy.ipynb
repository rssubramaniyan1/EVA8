{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aL5_mP_2mC9p",
        "bqTx0iVlnK-f",
        "LCkQA5jvu2fS",
        "2oUxktkqzmsC",
        "Slcdmyr92ZAW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rssubramaniyan1/EVA8/blob/main/EVA4_Session_2_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Import all the required libraries and modules for the program**"
      ],
      "metadata": {
        "id": "aL5_mP_2mC9p"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "source": [
        "from __future__ import print_function"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " This is the main module for the program. It contains all the required functions and classes for the program to run successfully and also contains the required functions for the program to run on GPU. It will be used to create the neural network and also to train the neural network and also to test the neural network for accuracy and loss values. This was built by Facebook AI Research (FAIR) and is used for deep learning applications."
      ],
      "metadata": {
        "id": "2moDr5QfmXt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "-J7qaJvfmXP2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the neural network module from torch. This is used to create the neural network model for the program. The neural network model is created by\n",
        "creating a class and then creating the required layers for the neural network model. The layers are created by using the torch.nn module.The layers contain the convolutional layers, maxpooling layers, fully connected layers, dropout layers, etc.\n"
      ],
      "metadata": {
        "id": "DXbJSVBWmlQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "N6kA5XREmulb"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing the functional module from torch. This is used to create the activation functions for the neural network model. The activation functions are\n",
        "created by using the torch.nn.functional module. The job of the activation functions is to introduce non-linearity into the neural network model.Non linearity is required for the neural network model to learn the complex patterns in the data. The activation functions used in this program are ReLU, Sigmoid, Tanh, etc.\n"
      ],
      "metadata": {
        "id": "wurZXP8xmyqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "eKCcuf9Am3G0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "importing the optim module from torch. This is used to create the optimizer for the neural network model. The optimizer is created by using the torch.optim module. the optim model includes SGD, Adam, RMSprop, etc. The optimizer is used to update the weights of the neural network model. The optimizer is used to minimize the loss function. The loss function is calculated by comparing the predicted output with the actual output. There different types of loss functions like MSE, CrossEntropy, etc."
      ],
      "metadata": {
        "id": "OHo9u7H1m5oN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "lTRbpVCjm-_y"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing the torchvision package. The torchvision module contains all the datasets like MNIST, CIFAR10, CIFAR100 etc. which we can use to train the neural network model.\n",
        "It also contains the transforms module which is used to transform the images. The transforms module contains the transforms like ToTensor, Normalize, etc. which are used to transform the images. The images are converted to tensors so that they can be used as input to the neural network model.\n",
        "The images are normalized so that the mean of the images is 0 and the standard deviation of the images is 1. This is done so that the neural network model can learn the patterns in the images better."
      ],
      "metadata": {
        "id": "dNOKFcuInBrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "t_Vp2UGznHxB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining the class Net which is used to create the neural network model.**\n",
        "\n",
        "The class Net is inherited from the nn.Module class. The nn.Module class is the base class for all the neural network modules in pytorch. The nn.Module class contains the functions and classes which are required to create the neural network model."
      ],
      "metadata": {
        "id": "bqTx0iVlnK-f"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    # defining the __init__ function which is used to initialize the class Net.\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # defining the convolutional layer 1 using the nn.Conv2d function.\n",
        "        # We pass 4 args to the nn.Conv2d function.\n",
        "        # The four args are :\n",
        "        #                   -->number of channels in the input image.\n",
        "        #                   -->number of kerne.\n",
        "        #                   -->size of the square convolution kernel.\n",
        "        #                   -->size of stride of the convolution. (The stride is the number of pixels by which\n",
        "        #                                                           the kernel is moved across the image.)\n",
        "\n",
        "        # The formula for calculating the parameters of the convolutional layer is :\n",
        "        # input channel*output channel*kernel size*kernel size + output channel\n",
        "\n",
        "        ########################################################################################################\n",
        "        # in conv layer1:\n",
        "        #_________________________________________________\n",
        "        # input channel = 1\n",
        "        # output channel = 32\n",
        "        # kernel size = 3*3\n",
        "        # stride = 1\n",
        "        # padding = 1\n",
        "        # total parameters = 1*32*3*3 + 32 = 320\n",
        "        # Receptive field = 3*3\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  #Input: 28x28|Output: 28x28|RF: 3x3 | 320 params\n",
        "\n",
        "        ########################################################################################################\n",
        "        # in conv layer2:\n",
        "        #_________________________________________________\n",
        "            # stride = 1\n",
        "            # padding = 1\n",
        "            # input channels = 32\n",
        "            # kernel size = 3x3\n",
        "            # output channels = 64\n",
        "            # Total Parameters: 32*3*3*64 + 64 = 18496\n",
        "            # Receptive Field: 5x5\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) #Input: 28x28 (x 32 channels) |Output: 28x28 (x 64 channels) |RF: 5x5 | 18496 params\n",
        "        \n",
        "        ########################################################################################################\n",
        "        # The maxpooling layer 1 is defined by using the nn.MaxPool2d function. We pass 2 args to the\n",
        "        # nn.MaxPool2d function. The two args are:\n",
        "        #                               -->size of the square window.\n",
        "        #                               -->size of stride of the window.\n",
        "        # this is done to reduce the number of parameters in the neural network model and also to reduce the computation.\n",
        "        # image size is reduced to 14x14 after the maxpooling layer 1. The reson for the reduction in the image size\n",
        "        # is the maxpooling layer reduces the size of the image by taking the maximum value over the size of the kernel.\n",
        "        # there is no loss of information in the maxpooling layer as the maximum value is taken over the size of the kernel.\n",
        "        #########################################################################################################\n",
        "        # in maxpool layer1:\n",
        "        #_________________________________________________\n",
        "\n",
        "            # kernel size = 2x2\n",
        "            # stride = 2\n",
        "            # Receptive Field: 10x10\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) #Input: 28x28 (x 64 channels) |Output: 14x14 (x 64 channels) |RF: 10x10\n",
        "        \n",
        "        ########################################################################################################\n",
        "        # in conv layer3:\n",
        "        #_________________________________________________\n",
        "                # stride = 1\n",
        "            # padding = 1\n",
        "            # input channels = 64\n",
        "            # kernel size = 3x3\n",
        "            # output channels = 128\n",
        "            # Total Parameters 64*3*3*128 + 128 = 73856\n",
        "            # Receptive Field: 12x12\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1) #Input: 14x14(x 64 channels) |Output: 14x14 (x 128 channels) |RF: 12x12 | 73856 params\n",
        "\n",
        "        ########################################################################################################\n",
        "        # in conv layer4:\n",
        "        #_________________________________________________\n",
        "                # stride=1\n",
        "                # padding=1\n",
        "                # input channels = 128\n",
        "                # kernel size = 3x3\n",
        "                # output channels = 256\n",
        "                # Total Parameters: 128*3*3*256 + 256 = 295168\n",
        "                # Receptive Field: 14x14\n",
        "\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1) #Input: 14x14 (x 128 channels) |Output: 14x14 (x 256 channels) |RF: 14x14  | 295168 params\n",
        "\n",
        "        ########################################################################################################\n",
        "        # in maxpool layer2:\n",
        "        #_________________________________________________\n",
        "            # kernel size = 2x2\n",
        "            # stride = 2\n",
        "            # Receptive Field: (14 * 14) *2 --> 28x28 The RF doubles after every maxpooling layer.\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) #Input: 14x14 (x 256 channels) |Output: 7x7 (x 256 channels) |RF: 28x28\n",
        "\n",
        "        ########################################################################################################\n",
        "        # in conv layer5:\n",
        "        #_________________________________________________\n",
        "                # stride=1\n",
        "                # stride = 1\n",
        "                # padding = 0\n",
        "                # input channels = 256\n",
        "                # kernel size = 3x3\n",
        "                # output channels = 512\n",
        "                # Total Parameters: 256*3*3*512 + 512 =1180160\n",
        "                # Receptive Field: 30x30\n",
        "\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3) #Input: 7x7 (x 256 channels) |Output: 5x5 (x 512 channels) |RF: 30x30 | 1180160 params\n",
        "        \n",
        "        ########################################################################################################\n",
        "        # in conv layer6:\n",
        "        #_________________________________________________\n",
        "\n",
        "                # stride = 1\n",
        "                # padding = 0\n",
        "                # input channels = 512\n",
        "                # kernel size = 3x3\n",
        "                # output channels = 1024\n",
        "                # Total Parameters: 512*3*3*1024 + 1024 = 4719616\n",
        "                # Receptive Field: 32x32\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3) #Input: 5x5 (x 512 channels) |Output: 3x3 (x 1024 channels) |RF: 32x32 | 4719616 params\n",
        "\n",
        "        ########################################################################################################\n",
        "        # in conv layer7:\n",
        "        #_________________________________________________\n",
        "                # stride = 1\n",
        "                # padding = 0\n",
        "                # input channels = 1024\n",
        "                # kernel size = 3x3\n",
        "                # output channels = 10\n",
        "                # Total Parameters: 1024*3*3*10+ 10 = 92170\n",
        "                # Receptive Field: 34x34\n",
        "\n",
        "            # The reason for the 10 channels is that we have 10 classes in the MNIST dataset.\n",
        "\n",
        "        self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "\n",
        "        ########################################################################################################\n",
        "        # the forward function:\n",
        "        #_________________________________________________\n",
        "        # the forward function is used to define the flow of the data through the neural network model. The data is passed\n",
        "        # through:\n",
        "        #        -->convolutional layer 1 (self.conv1) --> relu activation function --> convolutional layer 2 (self.conv2)\n",
        "        #        --> relu activation function --> maxpooling layer 1 (self.pool1) --> convolutional layer 3 (self.conv3)\n",
        "        #        --> relu activation function --> convolutional layer 4 (self.conv4) --> relu activation function\n",
        "        #        --> maxpooling layer 2 (self.pool2) --> convolutional layer 5 (self.conv5) --> relu activation function\n",
        "        #        --> convolutional layer 6 (self.conv6) --> relu activation function --> convolutional layer 7 (self.conv7)\n",
        "        #        --> relu activation function\n",
        "        #\n",
        "        #       the  convolution layer 7 has 10 output channels. The output of the convolutional layer 7 is passed through\n",
        "        #       the log_softmax function. The log_softmax function is used to calculate the log of the softmax function.\n",
        "        #       the softmax function is used to calculate the probability of the output of the neural network model. The\n",
        "        #       function returns the output of the log_softmax function. The output is the probability of the output of the\n",
        "        #      neural network model.\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "          x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "          x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "          x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "          x = F.relu(self.conv7(x))\n",
        "          x = x.view(-1, 10)\n",
        "          return F.log_softmax(x)\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data - Train & Test Set**"
      ],
      "metadata": {
        "id": "LCkQA5jvu2fS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The torchsummary module is used to print the summary of the neural network model. The summary includes the input size, output size, receptive field, number of parameters and the type of layer."
      ],
      "metadata": {
        "id": "_zvYUSZcyi3m"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f4d992-1eb5-477d-b652-ad07dcbf9123"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "use_cuda is used to check if the GPU is available. If the GPU is available, the model is moved to the GPU. If the GPU is not available, the model is moved to the CPU."
      ],
      "metadata": {
        "id": "E5TgyGVOyt4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "MEiWHq2jy1oi"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "device is used to move the model to the GPU or CPU."
      ],
      "metadata": {
        "id": "lufEeyVCy3hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "metadata": {
        "id": "Ev6V-n2xy9lK"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is defined by using the Net class and executed on the GPU or CPU."
      ],
      "metadata": {
        "id": "0v5PG5LZy_bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net().to(device)"
      ],
      "metadata": {
        "id": "cyRSR4rJzFH5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The summary of the model on the given input image which is 28*28 and 1 channel."
      ],
      "metadata": {
        "id": "OetZqq_dzG4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, input_size=(1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4ls6O5yzL1S",
        "outputId": "27b7b2aa-fa2c-45c2-f4dd-6a381caa0bf1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-3b34e34d9416>:165: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "source": [
        "# the manual_seed function is used to set the seed for generating random numbers. The seed is set to 1.\n",
        "# this is done to ensure that the results are reproducible.\n",
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "# the batch size is set to 128. The batch size is the number of images that are passed through the neural network\n",
        "# model at a time. this is done to reduce the computation time. larger batch sizes require more memory.\n",
        "# batch size is defined by using the batch_size variable. it is always a power of 2. the reason for this is that\n",
        "# the GPU is optimized for this. The GPU is used to speed up the training process. The GPU is used to perform\n",
        "# parallel processing on the batch size. If the batch size is not a power of 2, the memory is not used efficiently.\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "# the kwargs variable is used to define the number of workers. the number of workers is set to 1. the number of workers\n",
        "# is the number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\n",
        "# the number of workers is set to 1 because the dataset is small. if the dataset is large, the number of workers\n",
        "# should be set to 4 or 8. The pin_memory variable is set to True. this is done speed up the training process.\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "\n",
        "# the train_loader variable is used to load the training dataset. the dataset is loaded from the torchvision module.\n",
        "# the dataset is loaded from the MNIST function. the train variable is set to True. this is done to load the training\n",
        "# dataset. the transform variable is used to transform the images. the images are transformed to tensors. the images\n",
        "# are also normalized. the mean and standard deviation are set to 0.1307 and 0.3081 respectively. the batch_size\n",
        "# variable is used to define the batch size. the shuffle variable is set to True. this is done to shuffle the images\n",
        "# in the dataset. this is done to reduce the bias in the model. the dataset is loaded from the train_loader variable.\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "# the test_loader variable is used to load the test dataset. the dataset is loaded from the torchvision module.\n",
        "# the dataset is loaded from the MNIST function. the train variable is set to False. this is done to load the test\n",
        "# dataset. the transform variable is used to transform the images. the images are transformed to tensors. the images\n",
        "# are also normalized. the mean and standard deviation are set to 0.1307 and 0.3081 respectively. The mean and standard\n",
        "# deviation are the same as the training dataset which is calculated from the entire training dataset.\n",
        "# the batch_size variable is used to define the batch size. the shuffle variable is set to True.\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Define Model  Train and Test**"
      ],
      "metadata": {
        "id": "2oUxktkqzmsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the tqdm module is used to display the progress bar for the training and testing process."
      ],
      "metadata": {
        "id": "xn2mmDZw0TOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "fIY-BhX80RrV"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the train function. the train function is used to train the model. the train function takes the model,device, train_loader and optimizer and the epoch as input"
      ],
      "metadata": {
        "id": "jsAQwYgX0dm0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    \n",
        "    # for loop is used to iterate through the train_loader. the train_loader is \n",
        "    # used to load the training dataset.\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        \n",
        "        # the data and target are moved to the GPU or CPU.\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # the optimizer is set to zero. this is done to clear the gradients of the optimizer.\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # model of the data is stored in the output variable.\n",
        "        output = model(data)\n",
        "        \n",
        "        # the loss function is defined by using the negative log likelihood and stored in the loss variable.\n",
        "        # the negative log likelihood loss function is mathematically defined as:\n",
        "        # loss(x, class) = -x[class] + log(sum(exp(x[i]))) \n",
        "        # where x is the input tensor, class is the target class\n",
        "        # and i is the index of the class.\n",
        "\n",
        "        loss = F.nll_loss(output, target)\n",
        "\n",
        "        # the loss is backpropagated.\n",
        "\n",
        "        # What does backpropagation mean?\n",
        "        #\n",
        "        #   --> Backpropagation is the process of passing the loss backwards through the neural network model at each\n",
        "        # layer.\n",
        "        #   --> It is necessary to pass the loss backwards through the neural network model so that the gradients\n",
        "        # can be calculated.\n",
        "\n",
        "        # definition of gradient:\n",
        "        #   --> it is the rate of change of a function with respect to the parameters of the function.\n",
        "        #   --> it is the slope of the tangent line of the function.\n",
        "        #   --> it is the direction of the steepest ascent of the function.\n",
        "        # in this case, it is the rate of change of the loss function with respect to the\n",
        "        # parameters (weights and biases)\n",
        "\n",
        "        # bakcpropagation is used to calculate the gradients of the loss function. the gradients are used to\n",
        "        # update the weights and biases of the neural network model. using the chain rule we\n",
        "        # calculate the gradients of the loss function with respect to the weights and biases of the neural\n",
        "        # network model.\n",
        "        # The chain rule is mathematically defined as:\n",
        "        #   --> d(loss)/d(w) = d(loss)/d(output) * d(output)/d(w)\n",
        "        #   where\n",
        "        #       --> loss is the loss function,\n",
        "        #       --> output is the output of the neural network model,\n",
        "        #       --> w are the weights of the neural network model.\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # the optimizer is used to update the weights and biases of the neural network model. the optimizer is\n",
        "        # defined by using the stochastic gradient descent optimizer.\n",
        "        # how does the optimizer update the weights and biases?\n",
        "        #  --> the optimizer updates the weights and biases by using the gradients of the loss function with\n",
        "        #  respect to the weights and biases.\n",
        "        #  --> the optimizer updates the weights and biases by using the following equation:\n",
        "        #  --> w = w - learning_rate * gradient_of_loss_function_with_respect_to_w\n",
        "        #  --> b = b - learning_rate * gradient_of_loss_function_with_respect_to_b\n",
        "        #  where\n",
        "        #      --> w are the weights of the neural network model\n",
        "        #      --> b are the biases of the neural network model\n",
        "        #      --> learning_rate is the learning rate of the optimizer\n",
        "        #      --> gradient_of_loss_function_with_respect_to_w are the gradients of the loss function with respect to\n",
        "        #      the weights of the neural network model\n",
        "        #      --> the learning rate is used to control the step size while updating the weights and biases.\n",
        "        #      --> the learning rate is a hyperparameter of the neural network model.\n",
        "        #      --> too large of a learning rate can cause the loss function to diverge.\n",
        "        #      --> too small of a learning rate can cause the loss function to converge slowly.\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # the progress bar is updated. the progress bar is updated by using the following format:\n",
        "        #   --> loss={loss.item()} batch_id={batch_idx}\n",
        "        #   where\n",
        "        #       --> loss.item() is the loss of the current batch\n",
        "        #       --> batch_idx is the batch index of the current batch\n",
        "\n",
        "        pbar.set_description(desc=f'loss={loss.item()} batch_id={batch_idx}')\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test function is used to test the model. the test function takes the model, device and test_loader as input."
      ],
      "metadata": {
        "id": "2EMIEU-D2Bd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    # the model is set to evaluation mode. this is done to disable the dropout layer.\n",
        "    # the dropout layer is disabled during the testing process because we dont prevent for overfitting\n",
        "    # during the testing process. we only prevent for overfitting during the training process.\n",
        "    model.eval()\n",
        "\n",
        "    # the test_loss and correct variables are set to zero. the test_loss variable is used to store the loss of the\n",
        "    # testing process. the correct variable is used to store the number of correct predictions.\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # the torch.no_grad() context is used to disable the gradient calculation. the gradient calculation is\n",
        "    # disabled because we dont need the gradients during the testing process. the gradients are only needed\n",
        "    # during the training process. the gradients are used to update the weights and biases of the neural network\n",
        "    # model.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # for loop is used to iterate through the test_loader. the test_loader is used to load the testing dataset.\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            # the sum of the loss of the current batch is added to the test_loss variable. the loss of the current\n",
        "            # batch is calculated by using the negative log likelihood loss function.\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            # the prediction of the model is stored in the pred variable. the prediction of the model is calculated\n",
        "            # by using the argmax function. the argmax function is used to find the index of the maximum value in\n",
        "            # the output tensor. the index of the maximum value in the output tensor is the predicted class of the\n",
        "            # model. The argmax function is mathematically defined as:\n",
        "            #   --> argmax(x) = i where x[i] is the maximum value in the tensor x.\n",
        "            # the value in of the tensor x is the probability of the class.\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "\n",
        "            # the correct variable is updated by summing the number of correct predictions of the current batch\n",
        "            # with the correct variable.\n",
        "            # the correct variable is updated every batch. the final value of the correct variable is the total\n",
        "            # number of correct predictions. the total number of correct predictions is used to calculate the\n",
        "            # accuracy of the model. the accuracy of the model is calculated by dividing the total number of\n",
        "            # correct predictions by the total number of predictions. the total number of predictions is equal to\n",
        "            # the length of the test_loader. the length of the test_loader is equal to the number of batches in the\n",
        "            # test_loader.\n",
        "\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # the test_loss variable is updated by dividing the test_loss variable by the length of the test_loader. the\n",
        "    # length of the test_loader is equal to the number of batches in the test_loader. the test_loss variable is\n",
        "    # updated so that the test_loss variable is equal to the average loss of the testing process.\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    # the accuracy of the model is calculated by dividing the total number of correct predictions by the total\n",
        "    # number of predictions. the total number of predictions is equal to the length of the test_loader which is\n",
        "    # equal to the number of batches in the test_loader.\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "3lTVfWz71RVV"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Execution**\n",
        "The main function is used to run the code.\n",
        "In the below section of code we define the hyperparameters of the neural network model. the hyperparameters are defined by using the following variables:\n",
        "  --> epochs: the number of iterations of the training process over the train dataset. During each iteration of the training process the weights and biases of the neural network model are updated.\n",
        "\n",
        "  --> optimizer is used to update the weights and biases of the neural network model. the optimizer is defined by using the stochastic gradient descent optimizer. This is called from the torch.optim library."
      ],
      "metadata": {
        "id": "Slcdmyr92ZAW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a349ca7-c55d-4a13-ccf9-6bebc9f6ec98"
      },
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "for epoch in range(1,2):\n",
        "    # the train function is called to train the model. the train function takes the model, device, train_loader\n",
        "    # and optimizer as input. the train function is called for each epoch of the training process.\n",
        "\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "\n",
        "    # the test function is called to test the model. the test function takes the model, device and test_loader\n",
        "    # as input. the test function is called after each epoch of the training process.\n",
        "\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-45-3b34e34d9416>:165: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=1.9901642799377441 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 27.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9656, Accuracy: 2775/10000 (28%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}